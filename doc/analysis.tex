\section{Music Information Retrieval}

We use VOCANO \citep{vocano} as the note transcription model and conduct two sets of experiments in this task. Different training sets are used in two experiments, which are M4Singer and TONAS \citep{Mora2010} \citep{Bonada2013}, respectively. When training on M4Singer, we only use 1\% of the corpus to match the size of TONAS. MIR-1K \citep{mir1k} is used in both experiments as semi-supervised dataset. The model is then tested on an isolated subset of M4Singer. The training subset and the testing subset of M4Singer are sampled and split randomly in a 4:1 ratio and there is no data leak between them. As shown in the Table, the model trained using M4Singer outperforms the model of TONAS reasonably. We also present the experimental result of training VOCANO on TONAS and testing it on DALI \citep{meseguer2019dali}. This result shows a similar or slightly higher performance to the experiment where the training and testing are both done on M4Singer. This indicates that M4Singer is suitable for note transcription task. 

It is worth mentioning that the overall size of M4Singer is about 100 times larger than TONAS, which offers significant generalization potential for note transcription tasks. We only use 1\% of the M4Singer corpus to reach the performance discussed before. The performance of cross-set validation on note transcription is decreased more significantly than that on onset/offset detection. This might be because the pitch prediction introduce extra discrepancy. In addition, we find that VOCANO has a poor performance when it comes to bass singers (with low pitches). The performance of pitch prediction can be improved by 5\% by shifting the pitch of bass for 6-12 semitones.